{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Product Recognition of Food Products**\n",
        "\n",
        "## Image Processing and Computer Vision - Assignment Module \\#1\n",
        "\n",
        "\n",
        "Contacts:\n",
        "\n",
        "- Prof. Giuseppe Lisanti -> giuseppe.lisanti@unibo.it\n",
        "- Prof. Samuele Salti -> samuele.salti@unibo.it\n",
        "- Alex Costanzino -> alex.costanzino@unibo.it\n",
        "- Francesco Ballerini -> francesco.ballerini4@unibo.it\n",
        "\n",
        "\n",
        "Computer vision-based object detection techniques can be applied in super market settings to build a system that can identify products on store shelves.\n",
        "An example of how this system could be used would be to assist visually impaired customers or automate common store management tasks like detecting low-stock or misplaced products, given an image of a shelf in a store."
      ],
      "metadata": {
        "id": "MNBgGYg_lpVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task\n",
        "Develop a computer vision system that, given a reference image for each product, is able to identify such product from one picture of a store shelf.\n",
        "\n",
        "<figure>\n",
        "<a href=\"https://imgbb.com/\">\n",
        "  <center>\n",
        "  <img src=\"https://i.ibb.co/TwkMWnH/Screenshot-2024-04-04-at-14-54-51.png\" alt=\"Screenshot-2024-04-04-at-14-54-51\" border=\"0\" width=\"300\" />\n",
        "</a>\n",
        "</figure>\n",
        "\n",
        "For each type of product displayed in the\n",
        "shelf the system should report:\n",
        "1. Number of instances;\n",
        "1. Dimension of each instance (width and height in pixel of the bounding box that enclose them);\n",
        "1. Position in the image reference system of each instance (center of the bounding box that enclose them).\n",
        "\n",
        "#### Example of expected output\n",
        "```\n",
        "Product 0 - 2 instance found:\n",
        "  Instance 1 {position: (256, 328), width: 57px, height: 80px}\n",
        "  Instance 2 {position: (311, 328), width: 57px, height: 80px}\n",
        "Product 1 – 1 instance found:\n",
        ".\n",
        ".\n",
        ".\n",
        "```\n",
        "\n",
        "### Track A - Single Instance Detection\n",
        "Develop an object detection system to identify single instance of products given one reference image for each item and a scene image.\n",
        "\n",
        "The system should be able to correctly identify all the product in the shelves\n",
        "image.\n",
        "\n",
        "### Track B - Multiple Instances Detection\n",
        "In addition to what achieved at step A, the system should also be able to detect multiple instances of the same product."
      ],
      "metadata": {
        "id": "DW42NlZsyTv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "Two folders of images are provided:\n",
        "* **Models**: contains one reference image for each product that the system should be able to identify.\n",
        "* **Scenes**: contains different shelve pictures to test the developed algorithm in different scenarios. The images contained in this folder are corrupted by noise.\n",
        "\n",
        "#### Track A - Single Instance Detection\n",
        "* **Models**: {ref1.png to ref14.png}.\n",
        "* **Scenes**: {scene1.png to scene5.png}.\n",
        "\n",
        "#### Track B - Multiple Instances Detection\n",
        "* **Models**: {ref15.png to ref27.png}.\n",
        "* **Scenes**: {scene6.png to scene12.png}."
      ],
      "metadata": {
        "id": "9fIbZJKq16ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp -r /content/drive/MyDrive/AssignmentsIPCV/dataset.zip ./\n",
        "!unzip dataset.zip"
      ],
      "metadata": {
        "id": "NjP3GCdujYlw",
        "outputId": "16d7a2d4-3d4d-4df4-f903-c07e263e1a9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  dataset.zip\n",
            "   creating: dataset/\n",
            "  inflating: __MACOSX/._dataset      \n",
            "   creating: dataset/scenes/\n",
            "  inflating: __MACOSX/dataset/._scenes  \n",
            "  inflating: dataset/.DS_Store       \n",
            "  inflating: __MACOSX/dataset/._.DS_Store  \n",
            "   creating: dataset/models/\n",
            "  inflating: __MACOSX/dataset/._models  \n",
            "  inflating: dataset/scenes/scene12.png  \n",
            "  inflating: __MACOSX/dataset/scenes/._scene12.png  \n",
            "  inflating: dataset/scenes/scene10.png  \n",
            "  inflating: __MACOSX/dataset/scenes/._scene10.png  \n",
            "  inflating: dataset/scenes/scene11.png  \n",
            "  inflating: __MACOSX/dataset/scenes/._scene11.png  \n",
            "  inflating: dataset/scenes/scene5.png  \n",
            "  inflating: __MACOSX/dataset/scenes/._scene5.png  \n",
            "  inflating: dataset/scenes/scene4.png  \n",
            "  inflating: __MACOSX/dataset/scenes/._scene4.png  \n",
            "  inflating: dataset/scenes/scene6.png  \n",
            "  inflating: __MACOSX/dataset/scenes/._scene6.png  \n",
            "  inflating: dataset/scenes/scene7.png  \n",
            "  inflating: __MACOSX/dataset/scenes/._scene7.png  \n",
            "  inflating: dataset/scenes/scene3.png  \n",
            "  inflating: __MACOSX/dataset/scenes/._scene3.png  \n",
            "  inflating: dataset/scenes/scene2.png  \n",
            "  inflating: __MACOSX/dataset/scenes/._scene2.png  \n",
            "  inflating: dataset/scenes/scene1.png  \n",
            "  inflating: __MACOSX/dataset/scenes/._scene1.png  \n",
            "  inflating: dataset/scenes/scene9.png  \n",
            "  inflating: __MACOSX/dataset/scenes/._scene9.png  \n",
            "  inflating: dataset/scenes/scene8.png  \n",
            "  inflating: __MACOSX/dataset/scenes/._scene8.png  \n",
            "  inflating: dataset/models/ref8.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref8.png  \n",
            "  inflating: dataset/models/ref9.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref9.png  \n",
            "  inflating: dataset/models/ref12.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref12.png  \n",
            "  inflating: dataset/models/ref13.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref13.png  \n",
            "  inflating: dataset/models/ref11.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref11.png  \n",
            "  inflating: dataset/models/ref10.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref10.png  \n",
            "  inflating: dataset/models/ref14.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref14.png  \n",
            "  inflating: dataset/models/ref15.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref15.png  \n",
            "  inflating: dataset/models/ref17.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref17.png  \n",
            "  inflating: dataset/models/ref16.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref16.png  \n",
            "  inflating: dataset/models/ref27.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref27.png  \n",
            "  inflating: dataset/models/ref26.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref26.png  \n",
            "  inflating: dataset/models/ref18.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref18.png  \n",
            "  inflating: dataset/models/ref24.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref24.png  \n",
            "  inflating: dataset/models/ref25.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref25.png  \n",
            "  inflating: dataset/models/ref19.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref19.png  \n",
            "  inflating: dataset/models/ref21.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref21.png  \n",
            "  inflating: dataset/models/ref20.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref20.png  \n",
            "  inflating: dataset/models/ref22.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref22.png  \n",
            "  inflating: dataset/models/ref23.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref23.png  \n",
            "  inflating: dataset/models/ref7.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref7.png  \n",
            "  inflating: dataset/models/ref6.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref6.png  \n",
            "  inflating: dataset/models/ref4.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref4.png  \n",
            "  inflating: dataset/models/ref5.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref5.png  \n",
            "  inflating: dataset/models/ref1.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref1.png  \n",
            "  inflating: dataset/models/ref2.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref2.png  \n",
            "  inflating: dataset/models/ref3.png  \n",
            "  inflating: __MACOSX/dataset/models/._ref3.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation criteria\n",
        "1. **Procedural correctness**. There are several ways to solve the assignment. Design your own sound approach and justify every decision you make;\n",
        "\n",
        "2. **Clarity and conciseness**. Present your work in a readable way: format your code and comment every important step;\n",
        "\n",
        "3. **Correctness of results**. Try to solve as many instances as possible. You should be able to solve all the instances of the assignment, however, a thoroughly justified and sound procedure with a lower number of solved instances will be valued **more** than a poorly designed approach."
      ],
      "metadata": {
        "id": "5KRBeGbKsEDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import zipfile\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the ZIP file\n",
        "zip_file_path = '/content/drive/My Drive/dataset.zip'\n",
        "# Extraction directory\n",
        "extracted_folder = '/content/dataset/'\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder)\n",
        "\n",
        "# Path to the folder containing the reference images\n",
        "models_folder = os.path.join(extracted_folder, 'models')\n",
        "# Path to the folder containing the scene images\n",
        "scenes_folder = os.path.join(extracted_folder, 'scenes')\n",
        "\n",
        "# Function to extract SIFT features from an image\n",
        "def extract_sift_features(image_path):\n",
        "    # Read the image in grayscale\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    # Initialize the SIFT detector\n",
        "    sift = cv2.SIFT_create()\n",
        "    # Detect and describe features\n",
        "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
        "    return keypoints, descriptors\n",
        "\n",
        "# Function to perform feature matching between two sets of descriptors\n",
        "def match_features(descriptors1, descriptors2):\n",
        "    # Initialize the brute force matcher\n",
        "    bf = cv2.BFMatcher()\n",
        "    # Perform descriptor matching\n",
        "    matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
        "    # Apply ratio test to get the best matches\n",
        "    good_matches = []\n",
        "    for m, n in matches:\n",
        "        if m.distance < 0.75 * n.distance:\n",
        "            good_matches.append(m)\n",
        "    return good_matches\n",
        "\n",
        "# Function to calculate the position and size of each object instance in the scene\n",
        "def calculate_object_positions(matches, scene_keypoints, ref_keypoints):\n",
        "    # List to store the position and size of each object instance in the scene\n",
        "    object_positions = []\n",
        "    # Iterate over the rows of the match matrix\n",
        "    for i, row in enumerate(matches):\n",
        "        # Filter reference images that have at least one match\n",
        "        relevant_refs = np.where(row > 0)[0]\n",
        "        # Iterate over the relevant reference images\n",
        "        for ref_idx in relevant_refs:\n",
        "            # Get the corresponding keypoints from the scene and reference image\n",
        "            scene_kps = [scene_keypoints[i] for i in range(len(scene_keypoints)) if matches[i, ref_idx] > 0]\n",
        "            ref_kps = [ref_keypoints[ref_idx]]\n",
        "            # Calculate the homography between the corresponding keypoints\n",
        "            H, _ = cv2.findHomography(np.float32([kp.pt for kp in ref_kps]),\n",
        "                                       np.float32([kp.pt for kp in scene_kps]), cv2.RANSAC)\n",
        "            # Transform the reference image points to the scene\n",
        "            ref_corners = np.float32([[0, 0], [0, 1], [1, 1], [1, 0]]).reshape(-1, 1, 2)\n",
        "            transformed_corners = cv2.perspectiveTransform(ref_corners, H)\n",
        "            # Calculate the center and size of the bounding box enclosing the instance\n",
        "            x, y, w, h = cv2.boundingRect(transformed_corners)\n",
        "            center_x = x + w / 2\n",
        "            center_y = y + h / 2\n",
        "            size = max(w, h)\n",
        "            # Add the position and size of the instance to the list\n",
        "            object_positions.append((ref_idx + 1, center_x, center_y, size))\n",
        "    return object_positions\n",
        "\n",
        "# List to store the features of all reference images\n",
        "all_keypoints = []\n",
        "all_descriptors = []\n",
        "\n",
        "# List to store the feature matches for all scene images\n",
        "all_matches = []\n",
        "\n",
        "# Iterate over all images in the \"models\" folder\n",
        "for filename in os.listdir(models_folder):\n",
        "    if filename.endswith('.png'):\n",
        "        image_path = os.path.join(models_folder, filename)\n",
        "        # Extract SIFT features from the image\n",
        "        keypoints, descriptors = extract_sift_features(image_path)\n",
        "        # Add the features to the list\n",
        "        all_keypoints.append(keypoints)\n",
        "        all_descriptors.append(descriptors)\n",
        "\n",
        "# Iterate over all images in the \"scenes\" folder\n",
        "for filename in os.listdir(scenes_folder):\n",
        "    if filename.endswith('.png'):\n",
        "        scene_path = os.path.join(scenes_folder, filename)\n",
        "        # Extract SIFT features from the scene image\n",
        "        scene_keypoints, scene_descriptors = extract_sift_features(scene_path)\n",
        "        # List to store matches for this scene\n",
        "        scene_matches = []\n",
        "        # Iterate over the features of the reference images\n",
        "        for i, ref_descriptors in enumerate(all_descriptors):\n",
        "            # Perform feature matching between scene features and reference features\n",
        "            matches = match_features(scene_descriptors, ref_descriptors)\n",
        "            # Store the number of matches for this reference image\n",
        "            scene_matches.append(len(matches))\n",
        "        # Add matches for this scene to the general list\n",
        "        all_matches.append(scene_matches)\n",
        "\n",
        "# Convert the matches list to a numpy array for easier processing\n",
        "all_matches = np.array(all_matches)\n",
        "\n",
        "# List to store object positions for all scenes\n",
        "all_object_positions = []\n",
        "\n",
        "# Iterate over all scene images\n",
        "for i, filename in enumerate(os.listdir(scenes_folder)):\n",
        "    if filename.endswith('.png'):\n",
        "        scene_path = os.path.join(scenes_folder, filename)\n",
        "        # Extract SIFT features from the scene image\n",
        "        scene_keypoints, _ = extract_sift_features(scene_path)\n",
        "        # Calculate object positions in the scene\n",
        "        object_positions = calculate_object_positions(all_matches[i], scene_keypoints, all_keypoints)\n",
        "        # Add object positions for this scene to the general list\n",
        "        all_object_positions.append(object_positions)\n",
        "\n",
        "# Print object positions for each scene\n",
        "for i, object_positions in enumerate(all_object_positions):\n",
        "    print(f\"Scene {i + 1}:\")\n",
        "    for ref_idx, center_x, center_y, size in object_positions:\n",
        "        print(f\"  Product {ref_idx - 1} - 1 instance found:\")\n",
        "        print(f\"    Instance 1 {{position: ({int(center_x)}, {int(center_y)}), width: {int(size)}px, height: {int(size)}px}}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "WZ0mxRKqqDdW",
        "outputId": "c11f2baa-4a83-4c4b-8240-e0d3bd2b58af",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-a124e2f50e45>:52: DeprecationWarning: Calling nonzero on 0d arrays is deprecated, as it behaves surprisingly. Use `atleast_1d(cond).nonzero()` if the old behavior was intended. If the context of this warning is of the form `arr[nonzero(cond)]`, just use `arr[cond]`.\n",
            "  relevant_refs = np.where(row > 0)[0]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'numpy.int64' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a124e2f50e45>\u001b[0m in \u001b[0;36m<cell line: 117>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mscene_keypoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_sift_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# Calcular la posición de los objetos en la escena\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mobject_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_object_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_matches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscene_keypoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_keypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Agregar las posiciones de objetos de esta escena a la lista general\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mall_object_positions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-a124e2f50e45>\u001b[0m in \u001b[0;36mcalculate_object_positions\u001b[0;34m(matches, scene_keypoints, ref_keypoints)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mscene_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrelevant_refs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# Obtener el emparejamiento correspondiente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscene_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryIdx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mref_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0;31m# Obtener los puntos clave correspondientes de la escena y de la imagen de referencia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mscene_kp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscene_keypoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainIdx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.int64' object is not iterable"
          ]
        }
      ]
    }
  ]
}