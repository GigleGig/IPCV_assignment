{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20005,"status":"ok","timestamp":1720641058413,"user":{"displayName":"ABEL HILL","userId":"18195064169685092136"},"user_tz":-120},"id":"zaP5nphJMkCX","outputId":"d683515f-9b15-4220-bf68-c418b40a2ebd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Archive:  dataset.zip\n","   creating: dataset/\n","  inflating: __MACOSX/._dataset      \n","   creating: dataset/scenes/\n","  inflating: __MACOSX/dataset/._scenes  \n","  inflating: dataset/.DS_Store       \n","  inflating: __MACOSX/dataset/._.DS_Store  \n","   creating: dataset/models/\n","  inflating: __MACOSX/dataset/._models  \n","  inflating: dataset/scenes/scene12.png  \n","  inflating: __MACOSX/dataset/scenes/._scene12.png  \n","  inflating: dataset/scenes/scene10.png  \n","  inflating: __MACOSX/dataset/scenes/._scene10.png  \n","  inflating: dataset/scenes/scene11.png  \n","  inflating: __MACOSX/dataset/scenes/._scene11.png  \n","  inflating: dataset/scenes/scene5.png  \n","  inflating: __MACOSX/dataset/scenes/._scene5.png  \n","  inflating: dataset/scenes/scene4.png  \n","  inflating: __MACOSX/dataset/scenes/._scene4.png  \n","  inflating: dataset/scenes/scene6.png  \n","  inflating: __MACOSX/dataset/scenes/._scene6.png  \n","  inflating: dataset/scenes/scene7.png  \n","  inflating: __MACOSX/dataset/scenes/._scene7.png  \n","  inflating: dataset/scenes/scene3.png  \n","  inflating: __MACOSX/dataset/scenes/._scene3.png  \n","  inflating: dataset/scenes/scene2.png  \n","  inflating: __MACOSX/dataset/scenes/._scene2.png  \n","  inflating: dataset/scenes/scene1.png  \n","  inflating: __MACOSX/dataset/scenes/._scene1.png  \n","  inflating: dataset/scenes/scene9.png  \n","  inflating: __MACOSX/dataset/scenes/._scene9.png  \n","  inflating: dataset/scenes/scene8.png  \n","  inflating: __MACOSX/dataset/scenes/._scene8.png  \n","  inflating: dataset/models/ref8.png  \n","  inflating: __MACOSX/dataset/models/._ref8.png  \n","  inflating: dataset/models/ref9.png  \n","  inflating: __MACOSX/dataset/models/._ref9.png  \n","  inflating: dataset/models/ref12.png  \n","  inflating: __MACOSX/dataset/models/._ref12.png  \n","  inflating: dataset/models/ref13.png  \n","  inflating: __MACOSX/dataset/models/._ref13.png  \n","  inflating: dataset/models/ref11.png  \n","  inflating: __MACOSX/dataset/models/._ref11.png  \n","  inflating: dataset/models/ref10.png  \n","  inflating: __MACOSX/dataset/models/._ref10.png  \n","  inflating: dataset/models/ref14.png  \n","  inflating: __MACOSX/dataset/models/._ref14.png  \n","  inflating: dataset/models/ref15.png  \n","  inflating: __MACOSX/dataset/models/._ref15.png  \n","  inflating: dataset/models/ref17.png  \n","  inflating: __MACOSX/dataset/models/._ref17.png  \n","  inflating: dataset/models/ref16.png  \n","  inflating: __MACOSX/dataset/models/._ref16.png  \n","  inflating: dataset/models/ref27.png  \n","  inflating: __MACOSX/dataset/models/._ref27.png  \n","  inflating: dataset/models/ref26.png  \n","  inflating: __MACOSX/dataset/models/._ref26.png  \n","  inflating: dataset/models/ref18.png  \n","  inflating: __MACOSX/dataset/models/._ref18.png  \n","  inflating: dataset/models/ref24.png  \n","  inflating: __MACOSX/dataset/models/._ref24.png  \n","  inflating: dataset/models/ref25.png  \n","  inflating: __MACOSX/dataset/models/._ref25.png  \n","  inflating: dataset/models/ref19.png  \n","  inflating: __MACOSX/dataset/models/._ref19.png  \n","  inflating: dataset/models/ref21.png  \n","  inflating: __MACOSX/dataset/models/._ref21.png  \n","  inflating: dataset/models/ref20.png  \n","  inflating: __MACOSX/dataset/models/._ref20.png  \n","  inflating: dataset/models/ref22.png  \n","  inflating: __MACOSX/dataset/models/._ref22.png  \n","  inflating: dataset/models/ref23.png  \n","  inflating: __MACOSX/dataset/models/._ref23.png  \n","  inflating: dataset/models/ref7.png  \n","  inflating: __MACOSX/dataset/models/._ref7.png  \n","  inflating: dataset/models/ref6.png  \n","  inflating: __MACOSX/dataset/models/._ref6.png  \n","  inflating: dataset/models/ref4.png  \n","  inflating: __MACOSX/dataset/models/._ref4.png  \n","  inflating: dataset/models/ref5.png  \n","  inflating: __MACOSX/dataset/models/._ref5.png  \n","  inflating: dataset/models/ref1.png  \n","  inflating: __MACOSX/dataset/models/._ref1.png  \n","  inflating: dataset/models/ref2.png  \n","  inflating: __MACOSX/dataset/models/._ref2.png  \n","  inflating: dataset/models/ref3.png  \n","  inflating: __MACOSX/dataset/models/._ref3.png  \n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!cp -r /content/drive/MyDrive/LabSessionsIPCV/dataset.zip ./\n","!unzip dataset.zip"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1450,"status":"ok","timestamp":1720641075539,"user":{"displayName":"ABEL HILL","userId":"18195064169685092136"},"user_tz":-120},"id":"LmWg4nKXMvCx"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","# Paths to dataset directories\n","dataset_folder = 'dataset'\n","models_folder = os.path.join(dataset_folder, 'models')\n","scenes_folder = os.path.join(dataset_folder, 'scenes')\n","\n","# Load images\n","def load_images(folder, prefix, count):\n","    images = []\n","    for i in range(1, count + 1):\n","        path = os.path.join(folder, f'{prefix}{i}.png')\n","        image = cv2.imread(path)\n","        if image is not None:\n","            images.append(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","    return images\n","\n","ref_images = load_images(models_folder, 'ref', 14)\n","scene_images = load_images(scenes_folder, 'scene', 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"n-ynd3BsM1mJ","outputId":"09bff0b4-24eb-472c-a922-4ab32b3ea808"},"outputs":[],"source":["# Initialize SIFT detector\n","sift = cv2.SIFT_create(nfeatures=5000)\n","\n","# Function to detect and match features using SIFT and FLANN\n","def detect_and_match_features(ref_image, scene_image):\n","    # Detect and compute SIFT features\n","    kp1, des1 = sift.detectAndCompute(ref_image, None)\n","    kp2, des2 = sift.detectAndCompute(scene_image, None)\n","\n","    # FLANN based matcher\n","    FLANN_INDEX_KDTREE = 1\n","    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n","    search_params = dict(checks=100)\n","\n","    flann = cv2.FlannBasedMatcher(index_params, search_params)\n","    matches = flann.knnMatch(des1, des2, k=2)\n","\n","    # Apply ratio test\n","    good_matches = []\n","    for m, n in matches:\n","        if m.distance < 0.5 * n.distance:\n","            good_matches.append(m)\n","\n","    # Extract matched keypoints\n","    points1 = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 2)\n","    points2 = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 2)\n","\n","    # Apply RANSAC to filter out outliers\n","    if len(points1) >= 4:  # Minimum number of points needed to compute homography\n","        H, mask = cv2.findHomography(points1, points2, cv2.RANSAC, 3.0)\n","        matches_mask = mask.ravel().tolist()\n","    else:\n","        H, matches_mask = None, []\n","\n","    return kp1, kp2, good_matches, matches_mask, H\n","\n","# Function to find bounding boxes for matched features\n","def find_bounding_boxes(ref_image, scene_image, H):\n","    h, w = ref_image.shape[:2]\n","\n","    # Get bounding box corners\n","    corners = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1, 1, 2)\n","\n","    if H is not None:\n","        transformed_corners = cv2.perspectiveTransform(corners, H)\n","    else:\n","        transformed_corners = np.zeros((4, 1, 2), dtype=np.float32)\n","\n","    return transformed_corners\n","\n","# Function to check if bounding box contains sufficient context\n","def is_valid_bounding_box(bbox, scene_image):\n","    x_coords = bbox[:, 0, 0]\n","    y_coords = bbox[:, 0, 1]\n","\n","    if (np.min(x_coords) >= 0 and np.max(x_coords) < scene_image.shape[1] and\n","        np.min(y_coords) >= 0 and np.max(y_coords) < scene_image.shape[0]):\n","        area = cv2.contourArea(bbox)\n","        return area > 5000  # Minimum area threshold\n","    return False\n","\n","# Process each reference image against each scene image\n","for i, ref_image in enumerate(ref_images, 1):\n","    print(f'Processing Reference Image {i}')\n","    for j, scene_image in enumerate(scene_images, 1):\n","        kp1, kp2, good_matches, matches_mask, H = detect_and_match_features(ref_image, scene_image)\n","\n","        bounding_box_valid = False\n","        if len(good_matches) >= 10 and H is not None:\n","            bounding_box = find_bounding_boxes(ref_image, scene_image, H)\n","            bounding_box_valid = is_valid_bounding_box(bounding_box, scene_image)\n","\n","        # If not enough matches or invalid bounding box, prepare a dummy output\n","        if not bounding_box_valid:\n","            bounding_box = np.array([[[0, 0]], [[0, 0]], [[0, 0]], [[0, 0]]])\n","            print(f'Invalid bounding box or not enough matches for Scene {j} with Reference Image {i}')\n","\n","        # Draw bounding box on scene image\n","        scene_image_with_box = scene_image.copy()\n","        bounding_box = np.int32(bounding_box)\n","        cv2.polylines(scene_image_with_box, [bounding_box], True, (0, 255, 0), 3, cv2.LINE_AA)\n","\n","        # Draw matched keypoints\n","        draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n","                           singlePointColor = None,\n","                           matchesMask = matches_mask if bounding_box_valid else None, # draw only inliers\n","                           flags = 2)\n","        img_matches = cv2.drawMatches(ref_image, kp1, scene_image, kp2, good_matches, None, **draw_params)\n","\n","        # Display matches and bounding box\n","        plt.figure(figsize=(20, 10))\n","\n","        plt.subplot(1, 2, 1)\n","        plt.imshow(img_matches)\n","        plt.title(f'Matches for Scene {j}')\n","        plt.axis('off')\n","\n","        plt.subplot(1, 2, 2)\n","        plt.imshow(scene_image_with_box)\n","        plt.title(f'Bounding Box for Scene {j}')\n","        plt.axis('off')\n","\n","        plt.show()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyP1cak4SA3VwAW273V1RQqy","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
