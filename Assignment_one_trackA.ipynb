{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b40b9220-822c-46c1-a479-defa5da8ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Paths to dataset directories\n",
    "dataset_folder = 'dataset'\n",
    "models_folder = os.path.join(dataset_folder, 'models')\n",
    "scenes_folder = os.path.join(dataset_folder, 'scenes')\n",
    "\n",
    "# Load images\n",
    "def load_images(folder, prefix, count):\n",
    "    images = []\n",
    "    for i in range(1, count + 1):\n",
    "        path = os.path.join(folder, f'{prefix}{i}.png')\n",
    "        image = cv2.imread(path)\n",
    "        if image is not None:\n",
    "            images.append(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    return images\n",
    "\n",
    "ref_images = load_images(models_folder, 'ref', 14)\n",
    "scene_images = load_images(scenes_folder, 'scene', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64a8f66f-a2b6-4d81-aa45-5bf307b076a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoising function\n",
    "def Denoise(image):\n",
    "    # Convert to LAB color space\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    # Apply CLAHE to L-channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l)\n",
    "    limg = cv2.merge((cl, a, b))\n",
    "    image_enhanced = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n",
    "    \n",
    "    # Apply advanced denoising\n",
    "    image_denoised = cv2.fastNlMeansDenoisingColored(image_enhanced, None, 10, 10, 7, 21)\n",
    "    \n",
    "    return image_denoised\n",
    "\n",
    "scene_images_denoised = [Denoise(cv2.cvtColor(img, cv2.COLOR_RGB2BGR)) for img in scene_images]\n",
    "scene_images_denoised = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in scene_images_denoised]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7272417d-7fec-453e-8920-c62ca8062236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SIFT detector\n",
    "sift = cv2.SIFT_create(nfeatures=10000)\n",
    "\n",
    "# Function to detect and match features using SIFT and FLANN\n",
    "def detect_and_match_features(ref_image, scene_image):\n",
    "    # Detect and compute SIFT features\n",
    "    kp1, des1 = sift.detectAndCompute(ref_image, None)\n",
    "    kp2, des2 = sift.detectAndCompute(scene_image, None)\n",
    "    \n",
    "    # FLANN based matcher\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=200)\n",
    "    \n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(des1, des2, k=2)\n",
    "    \n",
    "    # Apply ratio test\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.6 * n.distance:\n",
    "            good_matches.append(m)\n",
    "    \n",
    "    # Extract matched keypoints\n",
    "    points1 = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 2)\n",
    "    points2 = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 2)\n",
    "    \n",
    "    # Apply RANSAC to filter out outliers\n",
    "    if len(points1) >= 4:  # Minimum number of points needed to compute homography\n",
    "        H, mask = cv2.findHomography(points1, points2, cv2.RANSAC, 3.0)\n",
    "        matches_mask = mask.ravel().tolist()\n",
    "    else:\n",
    "        H, matches_mask = None, []\n",
    "    \n",
    "    return kp1, kp2, good_matches, matches_mask, H\n",
    "\n",
    "# Function to find bounding boxes for matched features\n",
    "def find_bounding_boxes(ref_image, scene_image, H):\n",
    "    h, w = ref_image.shape[:2]\n",
    "    \n",
    "    # Get bounding box corners\n",
    "    corners = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1, 1, 2)\n",
    "    \n",
    "    if H is not None:\n",
    "        transformed_corners = cv2.perspectiveTransform(corners, H)\n",
    "    else:\n",
    "        transformed_corners = np.zeros((4, 1, 2), dtype=np.float32)\n",
    "    \n",
    "    return transformed_corners\n",
    "\n",
    "# Function to check if bounding box contains sufficient context\n",
    "def is_valid_bounding_box(bbox, scene_image):\n",
    "    x_coords = bbox[:, 0, 0]\n",
    "    y_coords = bbox[:, 0, 1]\n",
    "    \n",
    "    if (np.min(x_coords) >= 0 and np.max(x_coords) < scene_image.shape[1] and\n",
    "        np.min(y_coords) >= 0 and np.max(y_coords) < scene_image.shape[0]):\n",
    "        area = cv2.contourArea(bbox)\n",
    "        return area > 5000  # Minimum area threshold\n",
    "    return False\n",
    "\n",
    "# Function to calculate bounding box dimensions and center position\n",
    "def calculate_bbox_details(bbox):\n",
    "    x_coords = bbox[:, 0, 0]\n",
    "    y_coords = bbox[:, 0, 1]\n",
    "    width = int(np.max(x_coords) - np.min(x_coords))\n",
    "    height = int(np.max(y_coords) - np.min(y_coords))\n",
    "    center_x = int(np.min(x_coords) + width // 2)\n",
    "    center_y = int(np.min(y_coords) + height // 2)\n",
    "    return width, height, (center_x, center_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d12479f1-ecf8-4d44-bcd3-73db03488bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Invalid bounding box or not enough matches for Scene 2 with Reference Image 1\n",
      "  Invalid bounding box or not enough matches for Scene 3 with Reference Image 1\n",
      "  Invalid bounding box or not enough matches for Scene 5 with Reference Image 1\n",
      "Product 0 – 2 instance(s) found.\n",
      "  Instance 1 {position: (410, 538), width: 801px, height: 1049px}\n",
      "  Instance 2 {position: (887, 911), width: 380px, height: 516px}\n",
      "  Invalid bounding box or not enough matches for Scene 2 with Reference Image 2\n",
      "  Invalid bounding box or not enough matches for Scene 3 with Reference Image 2\n",
      "  Invalid bounding box or not enough matches for Scene 4 with Reference Image 2\n",
      "  Invalid bounding box or not enough matches for Scene 5 with Reference Image 2\n",
      "Product 1 – 1 instance(s) found.\n",
      "  Instance 1 {position: (1246, 532), width: 802px, height: 1049px}\n",
      "  Invalid bounding box or not enough matches for Scene 1 with Reference Image 3\n",
      "  Invalid bounding box or not enough matches for Scene 4 with Reference Image 3\n",
      "  Invalid bounding box or not enough matches for Scene 5 with Reference Image 3\n",
      "Product 2 – 2 instance(s) found.\n",
      "  Instance 1 {position: (304, 493), width: 603px, height: 940px}\n",
      "  Instance 2 {position: (1421, 369), width: 595px, height: 708px}\n",
      "  Invalid bounding box or not enough matches for Scene 1 with Reference Image 4\n",
      "  Invalid bounding box or not enough matches for Scene 3 with Reference Image 4\n",
      "  Invalid bounding box or not enough matches for Scene 4 with Reference Image 4\n",
      "  Invalid bounding box or not enough matches for Scene 5 with Reference Image 4\n",
      "Product 3 – 1 instance(s) found.\n",
      "  Instance 1 {position: (927, 482), width: 634px, height: 952px}\n",
      "  Invalid bounding box or not enough matches for Scene 1 with Reference Image 5\n",
      "  Invalid bounding box or not enough matches for Scene 3 with Reference Image 5\n",
      "  Invalid bounding box or not enough matches for Scene 4 with Reference Image 5\n",
      "  Invalid bounding box or not enough matches for Scene 5 with Reference Image 5\n",
      "Product 4 – 1 instance(s) found.\n",
      "  Instance 1 {position: (1528, 452), width: 561px, height: 851px}\n",
      "  Invalid bounding box or not enough matches for Scene 1 with Reference Image 6\n",
      "  Invalid bounding box or not enough matches for Scene 2 with Reference Image 6\n",
      "  Invalid bounding box or not enough matches for Scene 3 with Reference Image 6\n",
      "  Invalid bounding box or not enough matches for Scene 4 with Reference Image 6\n",
      "  Invalid bounding box or not enough matches for Scene 5 with Reference Image 6\n",
      "Product 5 – 0 instance(s) found.\n",
      "  Invalid bounding box or not enough matches for Scene 1 with Reference Image 7\n",
      "  Invalid bounding box or not enough matches for Scene 2 with Reference Image 7\n",
      "  Invalid bounding box or not enough matches for Scene 4 with Reference Image 7\n",
      "  Invalid bounding box or not enough matches for Scene 5 with Reference Image 7\n",
      "Product 6 – 1 instance(s) found.\n",
      "  Instance 1 {position: (892, 460), width: 537px, height: 654px}\n",
      "  Invalid bounding box or not enough matches for Scene 1 with Reference Image 8\n",
      "  Invalid bounding box or not enough matches for Scene 2 with Reference Image 8\n",
      "  Invalid bounding box or not enough matches for Scene 5 with Reference Image 8\n",
      "Product 7 – 2 instance(s) found.\n",
      "  Instance 1 {position: (1445, 398), width: 557px, height: 780px}\n",
      "  Instance 2 {position: (352, 260), width: 357px, height: 495px}\n",
      "  Invalid bounding box or not enough matches for Scene 1 with Reference Image 9\n",
      "  Invalid bounding box or not enough matches for Scene 2 with Reference Image 9\n",
      "  Invalid bounding box or not enough matches for Scene 3 with Reference Image 9\n",
      "  Invalid bounding box or not enough matches for Scene 4 with Reference Image 9\n",
      "  Invalid bounding box or not enough matches for Scene 5 with Reference Image 9\n",
      "Product 8 – 0 instance(s) found.\n",
      "  Invalid bounding box or not enough matches for Scene 1 with Reference Image 10\n",
      "  Invalid bounding box or not enough matches for Scene 2 with Reference Image 10\n",
      "  Invalid bounding box or not enough matches for Scene 3 with Reference Image 10\n",
      "  Invalid bounding box or not enough matches for Scene 5 with Reference Image 10\n",
      "Product 9 – 1 instance(s) found.\n",
      "  Instance 1 {position: (942, 291), width: 271px, height: 417px}\n",
      "  Invalid bounding box or not enough matches for Scene 1 with Reference Image 11\n",
      "  Invalid bounding box or not enough matches for Scene 2 with Reference Image 11\n",
      "  Invalid bounding box or not enough matches for Scene 3 with Reference Image 11\n",
      "  Invalid bounding box or not enough matches for Scene 4 with Reference Image 11\n",
      "Product 10 – 1 instance(s) found.\n",
      "  Instance 1 {position: (367, 751), width: 367px, height: 554px}\n",
      "  Invalid bounding box or not enough matches for Scene 1 with Reference Image 12\n",
      "  Invalid bounding box or not enough matches for Scene 2 with Reference Image 12\n",
      "  Invalid bounding box or not enough matches for Scene 3 with Reference Image 12\n",
      "  Invalid bounding box or not enough matches for Scene 4 with Reference Image 12\n",
      "Product 11 – 1 instance(s) found.\n",
      "  Instance 1 {position: (710, 743), width: 445px, height: 657px}\n",
      "  Invalid bounding box or not enough matches for Scene 1 with Reference Image 13\n",
      "  Invalid bounding box or not enough matches for Scene 2 with Reference Image 13\n",
      "  Invalid bounding box or not enough matches for Scene 3 with Reference Image 13\n",
      "  Invalid bounding box or not enough matches for Scene 4 with Reference Image 13\n",
      "  Invalid bounding box or not enough matches for Scene 5 with Reference Image 13\n",
      "Product 12 – 0 instance(s) found.\n",
      "  Invalid bounding box or not enough matches for Scene 1 with Reference Image 14\n",
      "  Invalid bounding box or not enough matches for Scene 2 with Reference Image 14\n",
      "  Invalid bounding box or not enough matches for Scene 3 with Reference Image 14\n",
      "  Invalid bounding box or not enough matches for Scene 4 with Reference Image 14\n",
      "  Invalid bounding box or not enough matches for Scene 5 with Reference Image 14\n",
      "Product 13 – 0 instance(s) found.\n"
     ]
    }
   ],
   "source": [
    "# Process each reference image against each scene image and output the results\n",
    "for i, ref_image in enumerate(ref_images, 1):\n",
    "    instance_details = []\n",
    "    instance_count = 0\n",
    "    for j, scene_image in enumerate(scene_images_denoised, 1):\n",
    "        kp1, kp2, good_matches, matches_mask, H = detect_and_match_features(ref_image, scene_image)\n",
    "        \n",
    "        bounding_box_valid = False\n",
    "        if len(good_matches) >= 10 and H is not None:\n",
    "            bounding_box = find_bounding_boxes(ref_image, scene_image, H)\n",
    "            bounding_box_valid = is_valid_bounding_box(bounding_box, scene_image)\n",
    "        \n",
    "        # If not enough matches or invalid bounding box, prepare a dummy output\n",
    "        if not bounding_box_valid:\n",
    "            bounding_box = np.array([[[0, 0]], [[0, 0]], [[0, 0]], [[0, 0]]])\n",
    "            print(f'  Invalid bounding box or not enough matches for Scene {j} with Reference Image {i}')\n",
    "        else:\n",
    "            # Calculate bounding box dimensions and center position\n",
    "            width, height, center = calculate_bbox_details(bounding_box)\n",
    "            instance_count += 1\n",
    "            instance_details.append(f'  Instance {instance_count} {{position: {center}, width: {width}px, height: {height}px}}')\n",
    "        \n",
    "        # Draw bounding box on scene image\n",
    "        scene_image_with_box = scene_image.copy()\n",
    "        bounding_box = np.int32(bounding_box)\n",
    "        cv2.polylines(scene_image_with_box, [bounding_box], True, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "        \n",
    "        # Draw matched keypoints\n",
    "        draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                           singlePointColor = None,\n",
    "                           matchesMask = matches_mask if bounding_box_valid else None, # draw only inliers\n",
    "                           flags = 2)\n",
    "        img_matches = cv2.drawMatches(ref_image, kp1, scene_image, kp2, good_matches, None, **draw_params)\n",
    "        \n",
    "        # # Display matches and bounding box\n",
    "        # plt.figure(figsize=(20, 10))\n",
    "        \n",
    "        # plt.subplot(1, 2, 1)\n",
    "        # plt.imshow(img_matches)\n",
    "        # plt.title(f'Matches for Scene {j}')\n",
    "        # plt.axis('off')\n",
    "        \n",
    "        # plt.subplot(1, 2, 2)\n",
    "        # plt.imshow(scene_image_with_box)\n",
    "        # plt.title(f'Bounding Box for Scene {j}')\n",
    "        # plt.axis('off')\n",
    "        \n",
    "        # plt.show()\n",
    "    \n",
    "    # Print the summary for the current product\n",
    "    print(f'Product {i - 1} – {instance_count} instance(s) found.')\n",
    "    for detail in instance_details:\n",
    "        print(detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527502de-a769-4983-be50-e33e9369cef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_pytorch_gpu",
   "language": "python",
   "name": "tf_pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
